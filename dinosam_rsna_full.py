# -*- coding: utf-8 -*-
"""trail 3 dataset 3 rsna .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r1xo4zQl-vJ1q_PVtiyWU1ZixCOIjb01
"""

from google.colab import drive
drive.mount('/content/drive')

import zipfile
zip_path = "/content/drive/MyDrive/rsna_png.zip"
extract_path = "/content/rsna_png"

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("✅ Extraction completed!")

import os, shutil
from glob import glob
from sklearn.model_selection import train_test_split

# Original extracted dataset
base = "/content/rsna_png"

# New balanced split
output = "/content/rsna_png_80_10_10"
os.makedirs(output, exist_ok=True)

# Create folders
for split in ["train", "val", "test"]:
    for c in ["normal", "pneumonia"]:
        os.makedirs(os.path.join(output, split, c), exist_ok=True)

# ---- Collect all images from current train + test ----
def collect_class(label):
    imgs = []
    for split in ["train", "test"]:
        for ext in ["*.png", "*.jpg", "*.jpeg"]:
            imgs += glob(os.path.join(base, split, label, ext))
    return imgs

normal_imgs    = collect_class("normal")
pneumonia_imgs = collect_class("pneumonia")

print("Total normal images:", len(normal_imgs))
print("Total pneumonia images:", len(pneumonia_imgs))

# ================================
# SPLIT: 80% train / 10% val / 10% test
# ================================

# ---- NORMAL ----
train_n, temp_n = train_test_split(normal_imgs, test_size=0.20, random_state=42, shuffle=True)
val_n, test_n   = train_test_split(temp_n,    test_size=0.50, random_state=42, shuffle=True)

# ---- PNEUMONIA ----
train_p, temp_p = train_test_split(pneumonia_imgs, test_size=0.20, random_state=42, shuffle=True)
val_p, test_p   = train_test_split(temp_p,        test_size=0.50, random_state=42, shuffle=True)

# ---- Copy files ----
def move(files, split, label):
    for f in files:
        dst = os.path.join(output, split, label, os.path.basename(f))
        shutil.copy(f, dst)

# TRAIN
move(train_n, "train", "normal")
move(train_p, "train", "pneumonia")

# VAL
move(val_n, "val", "normal")
move(val_p, "val", "pneumonia")

# TEST
move(test_n, "test", "normal")
move(test_p, "test", "pneumonia")

# Summary
print("\nSPLIT SUMMARY (80/10/10)")
print("----------------------------")
print(f"Train  | normal: {len(train_n)} | pneumonia: {len(train_p)} | total: {len(train_n)+len(train_p)}")
print(f"Val    | normal: {len(val_n)}   | pneumonia: {len(val_p)}   | total: {len(val_n)+len(val_p)}")
print(f"Test   | normal: {len(test_n)}  | pneumonia: {len(test_p)}  | total: {len(test_n)+len(test_p)}")
print("----------------------------")
print("New dataset path:", output)



############################################################
# BLOCK 0 – IMPORTS, SETUP, DEVICE, REPRODUCIBILITY
############################################################

# ----- Python & OS -----
import os, math, random, zipfile, shutil, warnings, ssl

# ----- Numerical -----
import numpy as np

# ----- Torch -----
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# ----- Vision -----
import torchvision.transforms as T
from PIL import Image

# ----- Models -----
import timm
from transformers import SamModel, SamProcessor

# ----- Metrics -----
from sklearn.metrics import (
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve,
    f1_score,
    accuracy_score
)

# ----- Visualization -----
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

# ----- Other -----
from tqdm.auto import tqdm         # best for notebooks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

# ----- Remove warnings -----
warnings.filterwarnings("ignore")

# ----- CUDA memory fix -----
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

# ----- Device -----
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# ----- Reproducibility -----
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# ----- Matplotlib clean style -----
plt.style.use("default")
plt.rcParams["figure.facecolor"] = "white"
plt.rcParams["axes.facecolor"] = "white"

import os
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

base_dir = "/content/rsna_png_80_10_10"

train_dir = os.path.join(base_dir, "train")
val_dir   = os.path.join(base_dir, "val")
test_dir  = os.path.join(base_dir, "test")

def show_images_and_counts(data_dir, title):
    print(f"\n=== {title.upper()} ===")

    classes = sorted(
        [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
    )

    for cls in classes:
        cls_path = os.path.join(data_dir, cls)
        images = [img for img in os.listdir(cls_path)
                  if img.lower().endswith(('.png', '.jpg', '.jpeg'))]

        print(f"Class '{cls}': {len(images)} images")

        sample_images = random.sample(images, min(10, len(images)))

        plt.figure(figsize=(12, 4))
        plt.suptitle(f"{title} - {cls}", fontsize=14)

        for i, img_name in enumerate(sample_images):
            img_path = os.path.join(cls_path, img_name)
            img = mpimg.imread(img_path)

            plt.subplot(2, 5, i + 1)
            plt.imshow(img, cmap='gray')
            plt.axis('off')

        plt.show()

# Run
show_images_and_counts(train_dir, "Train")
show_images_and_counts(val_dir, "Validation")
show_images_and_counts(test_dir, "Test")

"""## 1. Install Required Dependencies"""

!pip install transformers torch torchvision pillow scikit-learn matplotlib seaborn tqdm -q

############################################################
# BLOCK 1 – CONFIG (RSNA PNG 80/10/10)
############################################################
import os

class Config:
    # === Dataset Paths ===
    base_dir = "/content/rsna_png_80_10_10"   # <— RSNA dataset

    train_dir = os.path.join(base_dir, "train")
    val_dir   = os.path.join(base_dir, "val")
    test_dir  = os.path.join(base_dir, "test")

    # === Models ===
    # DINOv3 ViT-Small (timm) + SAM ViT-Base (HF)
    dino_model = "vit_small_patch16_dinov3"
    sam_model  = "facebook/sam-vit-base"

    # === Feature Dimensions ===
    dino_dim   = 384      # DINOv3-small hidden size
    sam_dim    = 256      # SAM ViT-base channels after encoder
    fusion_dim = dino_dim + sam_dim   # 384 + 256 = 640

    num_classes = 2       # normal vs pneumonia

    # === Training ===
    epochs       = 10
    lambda_mse   = 0.05     # λ in CE + λ * MSE(dino_prob, sam_prob)
    lr_head      = 1e-4
    lr_backbone  = 3e-5
    weight_decay = 0.05
    batch_size   = 1
    grad_clip    = 1.0

    # === Image Sizes ===
    img_size_dino = 224     # DINOv3 resolution
    img_size_sam  = 1024    # SAM required resolution

    use_class_weights = True


config = Config()
print("Configuration loaded.")
print("Train dir:", config.train_dir)
print("Val   dir:", config.val_dir)
print("Test  dir:", config.test_dir)

############################################################
# BLOCK 2 – PROCESSORS + AUGMENTATION
############################################################

print("Loading processors...")

# ---- 1) DINOv3 processor ----
class DinoV3Preprocessor:
    def __init__(self, img_size=224):
        self.img_size = img_size
        self.transform = T.Compose([
            T.Resize((img_size, img_size)),
            T.ToTensor(),
            T.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def __call__(self, images, return_tensors="pt"):
        x = self.transform(images)
        x = x.unsqueeze(0)
        return {"pixel_values": x}

dino_processor = DinoV3Preprocessor(img_size=config.img_size_dino)

# ---- 2) SAM processor ----
sam_processor = SamProcessor.from_pretrained(
    config.sam_model,
    size={"longest_edge": config.img_size_sam}
)

# ---- 3) Strong data augmentation (from the image) ----
train_transform = T.Compose([
    # 1. Horizontal flip
    T.RandomHorizontalFlip(p=0.5),

    # 2. Random rotation (5–15 degrees)
    T.RandomRotation(degrees=(-15, 15), fill=0),

    # 3. Random translation / shift
    T.RandomAffine(
        degrees=0,
        translate=(0.15, 0.15),   # up to 15% shift
        scale=None,
        fill=0
    ),

    # 4. Random zoom (scale 0.8–1.2 → approx 0.1–0.2 zoom)
    T.RandomAffine(
        degrees=0,
        scale=(0.8, 1.2),
        fill=0
    ),

    # 5. Random brightness (+/- 20%)
    T.ColorJitter(brightness=0.2, contrast=0.2),

    # 6. Gaussian noise
    T.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),
])



# No augmentation for validation/test
val_transform = None
test_transform = None

print("Processors and transforms ready.")

############################################################
# BLOCK 3 – DATASET (ChestXrayDualDataset – DINO + SAM)
############################################################

class ChestXrayDualDataset(Dataset):
    """
    Dataset that provides both DINO and SAM preprocessed inputs.

    Returns a dict:
        pixel_values_dino : Tensor [3, 224, 224]
        pixel_values_sam  : Tensor [3, 1024, 1024]
        label             : LongTensor scalar {0,1}
    """

    def __init__(self, data_dir, dino_processor, sam_processor,
                 sam_size=1024, transform=None):
        self.data_dir       = data_dir
        self.dino_processor = dino_processor
        self.sam_processor  = sam_processor
        self.sam_size       = sam_size
        self.transform      = transform

        # Load all image paths + labels
        self.samples = []
        classes = sorted(os.listdir(data_dir))   # e.g. ['fasle', 'true']
        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}

        for cls in classes:
            cls_path = os.path.join(data_dir, cls)
            if os.path.isdir(cls_path):
                for img_name in os.listdir(cls_path):
                    img_path = os.path.join(cls_path, img_name)
                    self.samples.append((img_path, self.class_to_idx[cls]))

        print(f"Loaded {len(self.samples)} images from {data_dir}")
        print(f"Classes: {self.class_to_idx}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]

        # 1) Load image (PIL)
        image = Image.open(img_path).convert("RGB")

        # 2) Augmentation (train only)
        if self.transform is not None:
            image = self.transform(image)

        # 3) DINO branch – [1, 3, 224, 224] -> squeeze -> [3, 224, 224]
        dino_inputs = self.dino_processor(
            images=image,
            return_tensors="pt"
        )
        pixel_values_dino = dino_inputs["pixel_values"].squeeze(0)

        # 4) SAM branch – [1, 3, 1024, 1024] -> squeeze -> [3, 1024, 1024]
        sam_inputs = self.sam_processor(
            images=image,
            return_tensors="pt",
            size={"longest_edge": self.sam_size}
        )
        pixel_values_sam = sam_inputs["pixel_values"].squeeze(0)

        return {
            "pixel_values_dino": pixel_values_dino,       # [3, 224, 224]
            "pixel_values_sam":  pixel_values_sam,        # [3, 1024, 1024]
            "label":             torch.tensor(label, dtype=torch.long)
        }

############################################################
# BLOCK 4 – CREATE DATASETS & DATALOADERS
############################################################

print("Creating datasets...")

# TRAIN: with augmentation
train_dataset = ChestXrayDualDataset(
    config.train_dir,
    dino_processor,
    sam_processor,
    sam_size=config.img_size_sam,
    transform=train_transform
)

# VAL: same images as train, but no augmentation
val_dataset = ChestXrayDualDataset(
    config.val_dir,
    dino_processor,
    sam_processor,
    sam_size=config.img_size_sam,
    transform=val_transform
)

# TEST: separate folder, no augmentation
test_dataset = ChestXrayDualDataset(
    config.test_dir,
    dino_processor,
    sam_processor,
    sam_size=config.img_size_sam,
    transform=test_transform
)

train_loader = DataLoader(
    train_dataset,
    batch_size=config.batch_size,
    shuffle=True,
    num_workers=0,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=config.batch_size,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=config.batch_size,
    shuffle=False,
    num_workers=0,
    pin_memory=True
)

print("\nDataset sizes:")
print(f"Train: {len(train_dataset)}")
print(f"Val  : {len(val_dataset)}")
print(f"Test : {len(test_dataset)}")

############################################################
# BLOCK 5 – CLASS WEIGHTS (IMBALANCED DATA)
############################################################
if config.use_class_weights:
    train_labels = [label for _, label in train_dataset.samples]
    class_counts = np.bincount(train_labels)  # [count_0, count_1]

    class_weights = 1.0 / class_counts
    class_weights = class_weights / class_weights.sum() * len(class_counts)
    class_weights = torch.FloatTensor(class_weights).to(device)

    print(f"Class counts (train): {class_counts}")
    print(f"Class weights: {class_weights.cpu().numpy()}")
else:
    class_weights = None
    print("Not using class weights.")

############################################################
# BLOCK 6 – DINOv3 BACKBONE (timm) – [B, 384, 14, 14]
############################################################

class DINOv3FeatureExtractor(nn.Module):
    """
    DINOv3 ViT-Small from timm.
    Input : x  [B, 3, 224, 224]
    Output: fm [B, 384, 14, 14]
    """
    def __init__(self):
        super().__init__()
        self.model = timm.create_model(
            "vit_small_patch16_dinov3",
            pretrained=True,
            num_classes=0
        )

        # Freeze all parameters first
        for p in self.model.parameters():
            p.requires_grad = False

        # Unfreeze last 3 transformer blocks
        for block in self.model.blocks[-3:]:
            for p in block.parameters():
                p.requires_grad = True

    def forward(self, x):
        # forward_features often returns [B, 384, 14, 14] for DINOv3
        tokens = self.model.forward_features(x)

        if isinstance(tokens, torch.Tensor):
            if tokens.dim() == 4:
                # Already [B, C, H, W]
                return tokens  # [B, 384, 14, 14]
            elif tokens.dim() == 3:
                # [B, N, C] – handle CLS token if present
                B, N, C = tokens.shape
                if N == 197:
                    patch_tokens = tokens[:, 1:, :]  # [B, 196, 384]
                else:
                    patch_tokens = tokens
                N_p = patch_tokens.shape[1]
                H = W = int(math.sqrt(N_p))
                patch_tokens = patch_tokens[:, :H*W, :]
                fm = patch_tokens.view(B, H, W, C).permute(0, 3, 1, 2)
                return fm
            else:
                raise RuntimeError(f"Unexpected DINO tokens shape: {tokens.shape}")
        else:
            raise RuntimeError("DINO forward_features did not return a Tensor.")

############################################################
# BLOCK 7 – SAM BACKBONE (facebook/sam-vit-base) – [B, 256, 14, 14]
############################################################

class SAMFeatureExtractor(nn.Module):
    """
    SAM-ViT-Base.
    Input : x  [B, 3, 1024, 1024]
    Output: fm [B, 256, 14, 14]
    """
    def __init__(self, model_name="facebook/sam-vit-base"):
        super().__init__()
        self.model = SamModel.from_pretrained(model_name)

        # Freeze SAM backbone
        for p in self.model.parameters():
            p.requires_grad = False

    def forward(self, x):
        # x: [B, 3, 1024, 1024]
        vision_out = self.model.vision_encoder(pixel_values=x)
        feats = vision_out.last_hidden_state  # typically [B, 256, H, W]

        if feats.dim() == 4:
            # e.g. [B, 256, 64, 64] → pool to [B, 256, 14, 14]
            fm = F.adaptive_avg_pool2d(feats, (14, 14))
            return fm
        else:
            raise RuntimeError(f"Unexpected SAM features shape: {feats.shape}")

############################################################
# BLOCK 8 – CBAM (CHANNEL + SPATIAL ATTENTION)
############################################################

class CBAM(nn.Module):
    """
    Convolutional Block Attention Module for 2D feature maps.
    Input : x [B, C, H, W]
    Output: x_att [B, C, H, W]
    """
    def __init__(self, channels, reduction=16):
        super().__init__()
        # Channel attention
        self.mlp = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels)
        )
        # Spatial attention
        self.spatial_conv = nn.Conv2d(
            in_channels=2,
            out_channels=1,
            kernel_size=7,
            padding=3
        )

    def forward(self, x):
        B, C, H, W = x.shape

        # ----- Channel attention -----
        avg_pool = F.adaptive_avg_pool2d(x, 1).view(B, C)
        max_pool = F.adaptive_max_pool2d(x, 1).view(B, C)

        mlp_avg = self.mlp(avg_pool)
        mlp_max = self.mlp(max_pool)

        channel_att = torch.sigmoid(mlp_avg + mlp_max).view(B, C, 1, 1)
        x = x * channel_att

        # ----- Spatial attention -----
        avg_sp = torch.mean(x, dim=1, keepdim=True)
        max_sp, _ = torch.max(x, dim=1, keepdim=True)
        sp_cat = torch.cat([avg_sp, max_sp], dim=1)  # [B, 2, H, W]

        spatial_att = torch.sigmoid(self.spatial_conv(sp_cat))  # [B, 1, H, W]
        x = x * spatial_att

        return x

############################################################
# BLOCK 9 – FUSION MODEL (DINOv3 + SAM + CBAM + 3 HEADS)
############################################################

class DinoSamFusionModel(nn.Module):
    """
    DINOv3 + SAM fusion with CBAM and three classifier heads.

    DINO  → fm_dino [B, 384, 14, 14]
    SAM   → fm_sam  [B, 256, 14, 14]
    Concat→ fused   [B, 640, 14, 14] → CBAM → [B, 640, 14, 14]

    Then:
      dino_vec  = GAP(fm_dino)  → [B, 384]
      sam_vec   = GAP(fm_sam)   → [B, 256]
      fused_vec = GAP(fused)    → [B, 640]

    Heads output:
      dino_logits  : [B, 2]
      sam_logits   : [B, 2]
      fused_logits : [B, 2]
    """
    def __init__(self, config):
        super().__init__()
        self.dino = DINOv3FeatureExtractor()
        self.sam  = SAMFeatureExtractor(config.sam_model)

        self.cbam = CBAM(channels=config.fusion_dim)   # 640
        self.gap  = nn.AdaptiveAvgPool2d((1, 1))

        self.head_fused = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.1),
            nn.Linear(config.fusion_dim, config.num_classes)
        )

        self.head_dino = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.1),
            nn.Linear(config.dino_dim, config.num_classes)
        )

        self.head_sam = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.1),
            nn.Linear(config.sam_dim, config.num_classes)
        )

    def forward(self, dino_img, sam_img):
        # Feature maps
        fm_dino = self.dino(dino_img)   # [B, 384, 14, 14]
        fm_sam  = self.sam(sam_img)     # [B, 256, 14, 14]

        fused = torch.cat([fm_dino, fm_sam], dim=1)   # [B, 640, 14, 14]
        fused = self.cbam(fused)                      # [B, 640, 14, 14]

        # GAP
        dino_vec  = self.gap(fm_dino).view(fm_dino.size(0), -1)   # [B, 384]
        sam_vec   = self.gap(fm_sam).view(fm_sam.size(0), -1)     # [B, 256]
        fused_vec = self.gap(fused).view(fused.size(0), -1)       # [B, 640]

        # Heads
        dino_logits  = self.head_dino(dino_vec)       # [B, 2]
        sam_logits   = self.head_sam(sam_vec)         # [B, 2]
        fused_logits = self.head_fused(fused_vec)     # [B, 2]

        # Return all three
        return fused_logits, dino_logits, sam_logits

"""# Add FocalLoss class"""

class FocalLoss(nn.Module):
    """
    Multi-class focal loss with optional per-class alpha weights.

    FL = - alpha_t * (1 - p_t)^gamma * log(p_t)
    """
    def __init__(self, alpha=None, gamma=2.0, reduction="mean"):
        super().__init__()
        self.gamma = gamma
        self.reduction = reduction
        self.alpha = alpha  # tensor [num_classes] or None

    def forward(self, logits, targets):
        # logits: [B, C], targets: [B]
        log_probs = F.log_softmax(logits, dim=1)   # [B, C]
        probs     = log_probs.exp()                # [B, C]

        one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()

        p_t     = (probs * one_hot).sum(dim=1)       # [B]
        log_p_t = (log_probs * one_hot).sum(dim=1)   # [B]

        if self.alpha is not None:
            alpha_t = self.alpha[targets]            # [B]
        else:
            alpha_t = 1.0

        loss = - alpha_t * (1.0 - p_t) ** self.gamma * log_p_t

        if self.reduction == "mean":
            return loss.mean()
        elif self.reduction == "sum":
            return loss.sum()
        else:
            return loss

############################################################
# BLOCK 10 – LOSS (CE + λ*MSE) & OPTIMIZER + FOCAL METRIC
############################################################

model = DinoSamFusionModel(config).to(device)
print("Model initialized on device.")

# Standard CE with class weights (for training)
criterion_ce = nn.CrossEntropyLoss(weight=class_weights)

# Focal loss (for monitoring only)
criterion_focal = FocalLoss(
    alpha=class_weights,   # same weights
    gamma=2.0,             # or config.focal_gamma
    reduction="mean"
)

def compute_total_loss(fused_logits, dino_logits, sam_logits, labels):
    """
    Train with:  L_total = CE + λ * MSE
    Also compute: focal_loss (separately, not used for backprop)
    """
    # 1) CE on fused logits  (training loss)
    ce_loss = criterion_ce(fused_logits, labels)

    # 2) Focal loss on fused logits  (for reporting)
    focal_loss = criterion_focal(fused_logits, labels)

    # 3) DINO–SAM consistency loss
    dino_prob = torch.softmax(dino_logits, dim=1)
    sam_prob  = torch.softmax(sam_logits, dim=1)
    mse_loss  = F.mse_loss(dino_prob, sam_prob)

    total_loss = ce_loss + config.lambda_mse * mse_loss
    return total_loss, ce_loss, mse_loss, focal_loss

# Optimizer: only parameters that require grad
trainable_params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.AdamW(
    trainable_params,
    lr=config.lr_head,
    weight_decay=config.weight_decay
)

print("Trainable parameters:", sum(p.numel() for p in trainable_params))

############################################################
# BLOCK 11 – TRAIN & EVAL FUNCTIONS
############################################################

def train_one_epoch(epoch):
    model.train()
    running_loss  = 0.0
    running_ce    = 0.0
    running_mse   = 0.0
    running_focal = 0.0
    correct = 0
    total   = 0

    for batch in tqdm(train_loader, desc=f"Train Epoch {epoch}"):
        dino_img = batch["pixel_values_dino"].to(device)  # [B, 3, 224, 224]
        sam_img  = batch["pixel_values_sam"].to(device)   # [B, 3, 1024, 1024]
        labels   = batch["label"].to(device)              # [B]

        fused_logits, dino_logits, sam_logits = model(dino_img, sam_img)

        # now returns 4 values (total, CE, MSE, focal)
        loss, ce_loss, mse_loss, focal_loss = compute_total_loss(
            fused_logits, dino_logits, sam_logits, labels
        )

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)
        optimizer.step()

        preds = fused_logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total   += labels.size(0)

        running_loss  += loss.item()
        running_ce    += ce_loss.item()
        running_mse   += mse_loss.item()
        running_focal += focal_loss.item()

    epoch_loss  = running_loss  / len(train_loader)
    epoch_ce    = running_ce    / len(train_loader)
    epoch_mse   = running_mse   / len(train_loader)
    epoch_focal = running_focal / len(train_loader)
    epoch_acc   = correct / total

    print(f"[Train] Epoch {epoch:02d} | "
          f"Loss={epoch_loss:.4f} (CE={epoch_ce:.4f}, "
          f"MSE={epoch_mse:.4f}, Focal={epoch_focal:.4f}) | "
          f"Acc={epoch_acc:.4f}")
    return epoch_loss, epoch_acc


def evaluate(loader, split_name="Val"):
    model.eval()
    running_loss  = 0.0
    running_focal = 0.0
    correct = 0
    total   = 0
    all_labels = []
    all_preds  = []
    all_probs1 = []

    with torch.no_grad():
        for batch in tqdm(loader, desc=f"Eval {split_name}"):
            dino_img = batch["pixel_values_dino"].to(device)
            sam_img  = batch["pixel_values_sam"].to(device)
            labels   = batch["label"].to(device)

            fused_logits, dino_logits, sam_logits = model(dino_img, sam_img)

            # again unpack 4 values
            loss, _, _, focal_loss = compute_total_loss(
                fused_logits, dino_logits, sam_logits, labels
            )

            probs = torch.softmax(fused_logits, dim=1)
            preds = probs.argmax(dim=1)

            running_loss  += loss.item()
            running_focal += focal_loss.item()
            correct += (preds == labels).sum().item()
            total   += labels.size(0)

            all_labels.extend(labels.cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_probs1.extend(probs[:, 1].cpu().numpy())

    epoch_loss  = running_loss  / len(loader)
    epoch_focal = running_focal / len(loader)
    epoch_acc   = correct / total
    f1   = f1_score(all_labels, all_preds, average="binary")
    auc  = roc_auc_score(all_labels, all_probs1)

    print(f"[{split_name}] Loss={epoch_loss:.4f} | Focal={epoch_focal:.4f} | "
          f"Acc={epoch_acc:.4f} | F1={f1:.4f} | AUC={auc:.4f}")

    return epoch_loss, epoch_acc, f1, auc, np.array(all_labels), np.array(all_preds)

model.eval()

# Get one batch from loader
batch = next(iter(train_loader))

dino_img = batch["pixel_values_dino"].to(device)   # [B, 3, 224, 224]
sam_img  = batch["pixel_values_sam"].to(device)    # [B, 3, 1024, 1024]
labels   = batch["label"].to(device)               # [B]

with torch.no_grad():
    fm_dino = model.dino(dino_img)   # [B, 384, 14, 14]
    fm_sam  = model.sam(sam_img)     # [B, 256, 14, 14]
    fused   = torch.cat([fm_dino, fm_sam], dim=1)

print("fm_dino:", fm_dino.shape)
print("fm_sam :", fm_sam.shape)
print("fused :", fused.shape)

from google.colab import drive
drive.mount('/content/drive')

# Create folder in Google Drive
save_dir = "/content/drive/MyDrive/trial 3 dataset 3 rsna"
os.makedirs(save_dir, exist_ok=True)

print("Saving checkpoints to:", save_dir)

############################################################
# BLOCK 12 – MAIN TRAINING LOOP (SAVE TO GOOGLE DRIVE)
############################################################

from datetime import datetime

save_dir = "/content/drive/MyDrive/trial 3 dataset 3 rsna"
os.makedirs(save_dir, exist_ok=True)

history = {
    "train_loss": [], "train_acc": [],
    "val_loss": [],   "val_acc": [],
    "val_f1": [],     "val_auc": []
}

best_val_f1 = 0.0
best_epoch  = 0

for epoch in range(1, config.epochs + 1):

    # 1) TRAIN
    train_loss, train_acc = train_one_epoch(epoch)

    # 2) VALIDATION  (evaluate returns val_f1 already)
    val_loss, val_acc, val_f1, val_auc, _, _ = evaluate(val_loader, "Val")

    # Save metrics
    history["train_loss"].append(train_loss)
    history["train_acc"].append(train_acc)
    history["val_loss"].append(val_loss)
    history["val_acc"].append(val_acc)
    history["val_f1"].append(val_f1)
    history["val_auc"].append(val_auc)

    print(f"Epoch {epoch}/{config.epochs} - "
          f"Train Loss: {train_loss:.4f}, Acc: {train_acc*100:.2f}% | "
          f"Val Loss: {val_loss:.4f},   Acc: {val_acc*100:.2f}%, "
          f"F1: {val_f1:.4f}, AUC: {val_auc:.4f}")

    # Save epoch checkpoint
    ckpt_path = f"{save_dir}/epoch_{epoch:02d}.pth"
    torch.save(model.state_dict(), ckpt_path)
    print(f"✓ Saved checkpoint: {ckpt_path}")

    # SAVE BEST MODEL BY F1 (not AUC)
    if val_f1 > best_val_f1:
        best_val_f1  = val_f1
        best_epoch   = epoch
        best_model_path = f"{save_dir}/best_model_f1.pth"
        torch.save(model.state_dict(), best_model_path)
        print(f"★ NEW BEST MODEL (by F1)! Saved to {best_model_path}")

print("\nTraining finished.")
print(f"Best model was epoch {best_epoch} with Val F1 = {best_val_f1:.4f}")

############################################################
# BLOCK 13 – FINAL EVALUATION ON TEST SET
############################################################

print("\n=== Final Evaluation on TEST set ===")
test_loss, test_acc, test_f1, test_auc, test_labels, test_preds = evaluate(
    test_loader, split_name="Test"
)

print("\n=== TEST METRICS ===")
print(f"Test Loss : {test_loss:.4f}")
print(f"Test Acc  : {test_acc*100:.2f}%")
print(f"Test F1   : {test_f1:.4f}")
print(f"Test AUC  : {test_auc:.4f}")

############################################################
# BLOCK 14 – PLOTS (CM, ROC, PR, TRAINING CURVES)
############################################################

import seaborn as sns
from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc

# ---------- 1) Confusion Matrix ----------
cm = confusion_matrix(test_labels, test_preds)

plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix – Test")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

## ---------- 2) ROC + AUC (recompute probs on Test) ----------
model.eval()
all_probs1 = []

with torch.no_grad():
    for batch in test_loader:
        dino_img = batch["pixel_values_dino"].to(device)
        sam_img  = batch["pixel_values_sam"].to(device)
        labels   = batch["label"].to(device)

        fused_logits, _, _ = model(dino_img, sam_img)

        probs  = torch.softmax(fused_logits, dim=1)
        all_probs1.extend(probs[:, 1].cpu().numpy())  # prob of class 1

all_probs1 = np.array(all_probs1)


fpr, tpr, _ = roc_curve(test_labels, all_probs1)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(5, 4))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.4f}")
plt.plot([0, 1], [0, 1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve – Test")
plt.legend()
plt.grid(True)
plt.show()

# ---------- 3) Precision–Recall Curve ----------
precision, recall, _ = precision_recall_curve(test_labels, all_probs1)

plt.figure(figsize=(5, 4))
plt.plot(recall, precision)
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision–Recall Curve – Test")
plt.grid(True)
plt.show()

"""# find the best threshold that gives the highest F1-score"""

# Load best model by F1
best_model_path =  "/content/drive/MyDrive/trial 3 dataset 3 rsna/best_model_f1.pth"

model = DinoSamFusionModel(config)
model.load_state_dict(torch.load(best_model_path, map_location=device))
model.to(device)
model.eval()

print("Best F1 model loaded!")

import numpy as np

all_probs = []
all_labels = []

with torch.no_grad():
    for batch in tqdm(val_loader, desc="Collecting validation predictions"):
        dino_img = batch["pixel_values_dino"].to(device)
        sam_img  = batch["pixel_values_sam"].to(device)
        labels   = batch["label"].to(device)

        fused_logits, _, _ = model(dino_img, sam_img)
        probs = torch.softmax(fused_logits, dim=1)[:, 1]  # probability of class 1

        all_probs.extend(probs.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

y_true = np.array(all_labels)
probs  = np.array(all_probs)

print("Done collecting probabilities!")

from sklearn.metrics import f1_score

thresholds = [0.30, 0.35, 0.40, 0.45, 0.50, 0.60]

best_f1 = 0
best_t  = 0

print("\nF1 scores at selected thresholds:\n")

for t in thresholds:
    preds = (probs >= t).astype(int)
    f1 = f1_score(y_true, preds)

    print(f"Threshold {t:.2f}: F1 = {f1:.4f}")

    if f1 > best_f1:
        best_f1 = f1
        best_t  = t

print("\n======================")
print(" Best Threshold Search ")
print("======================")
print(f"✅ Best Threshold: {best_t:.2f}")
print(f"✅ Best F1 Score: {best_f1:.4f}")

from sklearn.metrics import (
    precision_score, recall_score,
    f1_score, accuracy_score,
    confusion_matrix
)

threshold = 0.3

# Convert probabilities to binary predictions
preds = (probs >= threshold).astype(int)

# Compute metrics
precision = precision_score(y_true, preds)
recall    = recall_score(y_true, preds)
f1        = f1_score(y_true, preds)
accuracy  = accuracy_score(y_true, preds)
cm        = confusion_matrix(y_true, preds)

print(f"✅ Metrics at threshold {threshold}:")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-score  : {f1:.4f}")
print(f"Accuracy  : {accuracy:.4f}")
print("\nConfusion Matrix:")
print(cm)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score

# ROC curve
fpr, tpr, _ = roc_curve(y_true, probs)
roc_auc = roc_auc_score(y_true, probs)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.3f})")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Best F1 Model")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"✅ AUC: {roc_auc:.4f}")

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import seaborn as sns

best_threshold = best_t

y_pred = (probs >= best_threshold).astype(int)

cm = confusion_matrix(y_true, y_pred)
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1  = f1_score(y_true, y_pred)

print(f"\n✅ Metrics at threshold {best_threshold:.2f}:")
print(f"Accuracy  : {acc:.4f}")
print(f"Precision : {prec:.4f}")
print(f"Recall    : {rec:.4f}")
print(f"F1-score  : {f1:.4f}")

plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Class 0", "Class 1"],
            yticklabels=["Class 0", "Class 1"])
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.title(f"Confusion Matrix (threshold = {best_threshold:.2f})")
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt

# Compute curve
precisions, recalls, thresholds = precision_recall_curve(y_true, probs)
ap_score = average_precision_score(y_true, probs)

plt.figure(figsize=(8,6))
plt.plot(recalls, precisions, linewidth=2)
plt.xlabel("Recall", fontsize=14)
plt.ylabel("Precision", fontsize=14)
plt.title(f"Precision–Recall Curve (AP = {ap_score:.4f})", fontsize=16)
plt.grid(True)
plt.show()

"""# evaluate on test data thr = 0.3"""

import numpy as np
import torch
from tqdm.auto import tqdm

from sklearn.metrics import (
    f1_score, precision_score, recall_score, accuracy_score,
    confusion_matrix, roc_curve, roc_auc_score,
    precision_recall_curve, average_precision_score
)

import matplotlib.pyplot as plt
import seaborn as sns

# ============================
# 1) Load best F1 Dino+SAM model
# ============================
best_model_path = "/content/drive/MyDrive/trial 3 dataset 3 rsna/best_model_f1.pth"

model = DinoSamFusionModel(config)
state_dict = torch.load(best_model_path, map_location=device)
model.load_state_dict(state_dict)
model.to(device)
model.eval()

print("✅ Best F1 model loaded from:", best_model_path)

# ============================
# 2) Collect probabilities & labels on TEST set
# ============================
all_probs = []
all_labels = []

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Collecting TEST predictions"):
        dino_img = batch["pixel_values_dino"].to(device)
        sam_img  = batch["pixel_values_sam"].to(device)
        labels   = batch["label"].to(device)

        fused_logits, _, _ = model(dino_img, sam_img)
        probs = torch.softmax(fused_logits, dim=1)[:, 1]  # P(class=1)

        all_probs.extend(probs.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

y_true = np.array(all_labels)
probs  = np.array(all_probs)

print("✅ Done collecting TEST probabilities!")

# ============================
# 3) Metrics at fixed threshold = 0.30
# ============================
threshold = 0.30
preds = (probs >= threshold).astype(int)

precision = precision_score(y_true, preds)
recall    = recall_score(y_true, preds)
f1        = f1_score(y_true, preds)
accuracy  = accuracy_score(y_true, preds)
cm        = confusion_matrix(y_true, preds)

print(f"\n✅ Metrics on TEST at threshold {threshold}:")
print(f"Accuracy  : {accuracy:.4f}")
print(f"Precision : {precision:.4f}")
print(f"Recall    : {recall:.4f}")
print(f"F1-score  : {f1:.4f}")
print("\nConfusion Matrix:")
print(cm)

# ============================
# 4) ROC curve + AUC
# ============================
fpr, tpr, _ = roc_curve(y_true, probs)
roc_auc = roc_auc_score(y_true, probs)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.3f})")
plt.plot([0, 1], [0, 1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve – Dino+SAM (TEST)")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"✅ AUC (TEST): {roc_auc:.4f}")

# ============================
# 5) Confusion matrix (heatmap)
# ============================
plt.figure(figsize=(5, 4))
sns.heatmap(
    cm, annot=True, fmt="d", cmap="Blues",
    xticklabels=["Class 0", "Class 1"],
    yticklabels=["Class 0", "Class 1"]
)
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.title(f"Confusion Matrix (TEST, thr = {threshold:.2f})")
plt.tight_layout()
plt.show()

# ============================
# 6) Precision–Recall curve
# ============================
precisions, recalls, _ = precision_recall_curve(y_true, probs)
ap_score = average_precision_score(y_true, probs)

plt.figure(figsize=(8, 6))
plt.plot(recalls, precisions, linewidth=2)
plt.xlabel("Recall", fontsize=14)
plt.ylabel("Precision", fontsize=14)
plt.title(f"Precision–Recall Curve (TEST, AP = {ap_score:.4f})", fontsize=16)
plt.grid(True)
plt.show()

